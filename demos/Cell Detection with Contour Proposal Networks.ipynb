{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Detection with Contour Proposal Networks\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/bbbc039-cpn-u22-demo-arrow.png\" />\n",
    "\n",
    "In this tutorial you will learn how to use [Contour Proposal Networks](https://arxiv.org/abs/2104.03393) (CPN) for Cell Detection with the \n",
    "[BBBC039](https://bbbc.broadinstitute.org/BBBC039) dataset and [PyTorch](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "## Really quick: What is a Contour Proposal Network?\n",
    "For a given image the Contour Proposal Network proposes **pixel-precise object contours**, each outlining an entire object. Like other detection networks, the CPN uses a so called backbone network to compute its features. This could be for example a ResNet-FPN ([Feature Pyramid Network](https://arxiv.org/abs/1612.03144) with [ResNet](https://arxiv.org/abs/1512.03385)), or a standard [U-Net](https://arxiv.org/abs/1505.04597) to name just two of many other options.\n",
    "\n",
    "So it works kind of like bounding box regression models, such as [YOLO](https://arxiv.org/pdf/1506.02640.pdf) or [Region Proposal Networks](https://arxiv.org/abs/1506.01497), but with contours.\n",
    "\n",
    "You can find a detailed description on arXiv: https://arxiv.org/abs/2104.03393.\n",
    "\n",
    "## What is covered here?\n",
    "1. [Install source](#1.-Install-pip-install-celldetection)\n",
    "2. [The configuration](#2.-The-configuration)\n",
    "3. [The data](#3.-The-data)\n",
    "4. [The Contour Proposal Network](#4.-The-Contour-Proposal-Network)\n",
    "5. [Training](#5.-Training)\n",
    "6. [Evaluation and FPS measurement](#Testing-and-inference-speed)\n",
    "7. [Conclusion](#7.-Conclusion)\n",
    "\n",
    "## Automatic Mixed Precision (AMP)\n",
    "This tutorial includes support for PyTorch's [**Automatic Mixed Precision package**](https://pytorch.org/docs/stable/amp.html). You can enable and disable it in [section 2](#2.-The-configuration) (`conf.amp`) of this tutorial.\n",
    "If enabled, it will automatically select certain parts of the model to use *float16*. Some operations, like convolutions, are much faster this way. AMP can be applied both during training and testing.\n",
    "\n",
    "## 1. Install `pip install celldetection`\n",
    "\n",
    "<hr/>\n",
    "\n",
    "First make sure that you have installed [PyTorch](https://pytorch.org/get-started/locally). Follow the link for more information.\n",
    "\n",
    "To install `CellDetection`, you can simply use pip: `pip install celldetection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import albumentations as A\n",
    "import celldetection as cd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The configuration\n",
    "<hr/>\n",
    "The most important settings are stored in this `Config` object. You can save it with `conf.to_json('config.json')` and load it with `cd.Config.from_json('config.json')`. This is especially helpful if you plan to test many different configurations.\n",
    "\n",
    "The config includes among other things the optimizer choice, batch size, specific CPN architecture and settings, crop size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = cd.Config(\n",
    "    # data\n",
    "    directory='./data',\n",
    "    download_data=True,\n",
    "    in_channels=1,\n",
    "    classes=2,\n",
    "    shuffle=True,\n",
    "    bg_fg_dists=(0.8, 0.85),\n",
    "    crop_size=(256, 256),\n",
    "    \n",
    "    # cpn\n",
    "    cpn='CpnU22',  # see https://git.io/JOnWX for alternatives\n",
    "    score_thresh=.9,\n",
    "    val_score_threshs=(.6, .8, .9),\n",
    "    nms_thresh=.5,\n",
    "    val_nms_threshs=(.3, .5, .8),\n",
    "    contour_head_stride=2,\n",
    "    order=6,  # the higher, the more complex shapes can be detected\n",
    "    samples=64,  # number of coordinates per contour\n",
    "    refinement_iterations=3,\n",
    "    refinement_buckets=6,\n",
    "    inputs_mean=.5,\n",
    "    inputs_std=.5,\n",
    "    tweaks={\n",
    "        'BatchNorm2d': {'momentum': 0.05}\n",
    "    },\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer={'Adam': {'lr': 0.0008, 'betas': (0.9, 0.999)}},\n",
    "    scheduler={'StepLR': {'step_size': 5, 'gamma': .99}},\n",
    "    \n",
    "    # training\n",
    "    epochs=100,\n",
    "    steps_per_epoch=512,\n",
    "    batch_size=8,\n",
    "    amp=torch.cuda.is_available(),  # Automatic Mixed Precision (https://pytorch.org/docs/stable/amp.html)\n",
    "    \n",
    "    # testing\n",
    "    test_batch_size=4,\n",
    "    \n",
    "    # misc\n",
    "    num_workers=0,  #8 * int(os.name != 'nt'),\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The data\n",
    "<hr/>\n",
    "\n",
    "### Loading data\n",
    "Here, we load the data. If you already have a local copy of the BBBC039 dataset make sure to include the correct directory in the config and to disable the download option.\n",
    "Otherwise, the following code will automatically download the data from the Broad Institute: https://bbbc.broadinstitute.org/BBBC039."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bbbc039 = cd.data.BBBC039Train(conf.directory, download=conf.download_data)\n",
    "val_bbbc039 = cd.data.BBBC039Val(conf.directory)\n",
    "test_bbbc039 = cd.data.BBBC039Test(conf.directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations with [Albumentations](https://albumentations.ai/)\n",
    "\n",
    "Data augmentation is a very popular strategy to improve training. Below you find a very basic setup, feel free to test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Transpose(),\n",
    "    A.RandomGamma((42, 100)),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "    ], p=0.2),\n",
    "    A.OpticalDistortion(shift_limit=0.01, p=0.3, interpolation=0, border_mode=0),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_transforms(img, lbl, name=None):\n",
    "    img = cd.data.normalize_percentile(img.copy().squeeze(), percentile=99.8)\n",
    "    lbl = lbl.copy()\n",
    "    \n",
    "    # Show original\n",
    "    cd.vis.show_detection(img, contours=cd.data.labels2contours(lbl), contour_linestyle='-')\n",
    "    if name is not None:\n",
    "        plt.title(name)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show transformed\n",
    "    s = 3\n",
    "    plt.figure(None, (s * 9, s * 9))\n",
    "    for i in range(1, s * s + 1):\n",
    "        plt.subplot(s, s, i)\n",
    "        trans = transforms(image=img, mask=lbl.astype('int32'))\n",
    "        t_img, t_lbl = trans['image'], trans['mask']\n",
    "        cd.data.relabel_(t_lbl[..., None])\n",
    "        plt.title(f'transformed {t_img.dtype.name, t_img.shape, t_lbl.dtype.name, t_lbl.shape}')\n",
    "        cd.vis.show_detection(t_img, contours=cd.data.labels2contours(t_lbl), contour_linestyle='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, img, _, lbl = train_bbbc039[0]\n",
    "demo_transforms(img, lbl, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data, config, transforms=None, items=None, size=None):\n",
    "        self.transforms = transforms\n",
    "        self.gen = cd.data.CPNTargetGenerator(\n",
    "            samples=config.samples,\n",
    "            order=config.order,\n",
    "            max_bg_dist=config.bg_fg_dists[0],\n",
    "            min_fg_dist=config.bg_fg_dists[1],\n",
    "        )\n",
    "        self._items = items or len(data)\n",
    "        self.data = data\n",
    "        self.size = size\n",
    "        self.channels = config.in_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._items\n",
    "\n",
    "    @staticmethod\n",
    "    def map(image):\n",
    "        image = image / 255\n",
    "        if image.ndim == 2:\n",
    "            image = image[..., None]\n",
    "        return image.astype('float32')\n",
    "\n",
    "    @staticmethod\n",
    "    def unmap(image):\n",
    "        image = image * 255\n",
    "        image = np.clip(image, 0, 255).astype('uint8')\n",
    "        if image.ndim == 3 and image.shape[2] == 1:\n",
    "            image = np.squeeze(image, 2)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item >= len(self):\n",
    "            raise IndexError('Index out of bounds.')\n",
    "        item = item % len(self.data)\n",
    "        \n",
    "        # Get image and labels\n",
    "        name, img, _, labels = self.data[item]\n",
    "        \n",
    "        # Normalize intensities\n",
    "        img, labels = np.copy(img).squeeze(), np.copy(labels)\n",
    "        img = cd.data.normalize_percentile(img, percentile=99.8)\n",
    "        labels = labels.astype('int32')       \n",
    "        \n",
    "        # Optionally crop\n",
    "        if self.size is not None:\n",
    "            h, w = self.size\n",
    "            img, labels = cd.data.random_crop(img, labels, height=h, width=w)\n",
    "        \n",
    "        # Optionally transform\n",
    "        if self.transforms is not None:\n",
    "            r = self.transforms(image=img, mask=labels)\n",
    "            img, labels = r['image'], r['mask']\n",
    "\n",
    "        # Ensure channels exist\n",
    "        if labels.ndim == 2:\n",
    "            labels = labels[..., None]\n",
    "        \n",
    "        # Relabel to ensure that N objects are marked with integers 1..N\n",
    "        cd.data.relabel_(labels)\n",
    "        \n",
    "        # Feed labels to target generator\n",
    "        gen = self.gen\n",
    "        gen.feed(labels=labels)\n",
    "        \n",
    "        # Map image to range -1..1\n",
    "        image = self.map(img)\n",
    "        \n",
    "        # Return as dictionary\n",
    "        return OrderedDict({\n",
    "            'inputs': image,\n",
    "            'labels': gen.reduced_labels,\n",
    "            'fourier': (gen.fourier.astype('float32'),),\n",
    "            'locations': (gen.locations.astype('float32'),),\n",
    "            'sampled_contours': (gen.sampled_contours.astype('float32'),),\n",
    "            'sampling': (gen.sampling.astype('float32'),),\n",
    "            'targets': labels\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(train_bbbc039, conf, transforms, items=conf.steps_per_epoch * conf.batch_size,\n",
    "                  size=conf.crop_size)\n",
    "val_data = Data(val_bbbc039, conf)\n",
    "test_data = Data(test_bbbc039, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=conf.batch_size, num_workers=conf.num_workers,\n",
    "                          collate_fn=cd.universal_dict_collate_fn, shuffle=conf.shuffle)\n",
    "val_loader = DataLoader(val_data, batch_size=conf.test_batch_size, collate_fn=cd.universal_dict_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=cd.universal_dict_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Here, we take a look at the data. What you see here, is what the network will learn to produce.\n",
    "Specifically, we can see if the data is processed correctly, the intensity range is adequate, and also visualize the effect of the Contour Proposal Network's `order` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(data_loader, figsize=(8, 4.5)):\n",
    "    # Pick example\n",
    "    example = cd.asnumpy(next(iter(data_loader)))\n",
    "    contours = example['sampled_contours'][0]\n",
    "    image = cd.data.channels_first2channels_last(example['inputs'][0])\n",
    "    # Plot example\n",
    "    cd.vis.show_detection(image, contours=contours, figsize=figsize, contour_linestyle='-',\n",
    "                          cmap='gray' if image.shape[2] == 1 else ...)\n",
    "    plt.colorbar()\n",
    "    plt.ylim([0, image.shape[0]])\n",
    "    plt.xlim([0, image.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_plot(data, data_loader):\n",
    "    # Plot example data for different `order` settings\n",
    "    gen = data.gen\n",
    "    s = int(np.ceil(np.sqrt(conf.order)))\n",
    "    plt.figure(None, (s * 12, s * 6.75))\n",
    "    for gen.order in range(1, conf.order + 1):\n",
    "        plt.subplot(s, s, gen.order)\n",
    "        plot_example(data_loader, figsize=None)\n",
    "        plt.title(f'{\"Chosen o\" if gen.order == conf.order else \"O\"}rder: {gen.order}')\n",
    "    plt.show()\n",
    "    assert gen.order == conf.order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data with different `order` settings\n",
    "\n",
    "The `order` hyperparameter controls how complicated the contours can be.\n",
    "While `order = 1` only yields ellipses, higher settings add more and more detail.\n",
    "Notably, you can always reduce the `order` after training a CPN. However, increasing it requires additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_plot(val_data, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Contour Proposal Network\n",
    "\n",
    "<hr/>\n",
    "\n",
    "This code snippet defines the actual model and allows for some tweaks to its settings, as specified in the config.\n",
    "Here, `conf.cpn` can either be a class name, a file name of a PyTorch checkpoint, a url of a PyTorch checkpoint, or the name of a *pretrained model* hosted by `celldetection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.cpn in dir(cd.models):\n",
    "    model = getattr(cd.models, conf.cpn)(\n",
    "        in_channels=conf.in_channels, order=conf.order, samples=conf.samples,\n",
    "        refinement_iterations=conf.refinement_iterations, nms_thresh=conf.nms_thresh,\n",
    "        score_thresh=conf.score_thresh, contour_head_stride=conf.contour_head_stride,\n",
    "        classes=conf.classes, refinement_buckets=conf.refinement_buckets,\n",
    "        backbone_kwargs=dict(inputs_mean=conf.inputs_mean, inputs_std=conf.inputs_std)\n",
    "    ).to(conf.device)\n",
    "elif os.path.isfile(conf.cpn):\n",
    "    model = torch.load(conf.cpn, map_location=conf.device)\n",
    "else:\n",
    "    model = cd.fetch_model(conf.cpn, map_location=conf.device)\n",
    "if conf.tweaks is not None:\n",
    "    cd.conf2tweaks_(conf.tweaks, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### Optimizer, scheduler and gradient scaler\n",
    "\n",
    "Define the optimizer and scheduler according to the config.\n",
    "The gradient scaler only used when automated mixed precision is enabled for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = cd.conf2optimizer(conf.optimizer, model.parameters())\n",
    "scheduler = cd.conf2scheduler(conf.scheduler, optimizer)\n",
    "scaler = GradScaler() if conf.amp else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, device, optimizer, desc=None, scaler=None, scheduler=None, progress=True):\n",
    "    model.train()\n",
    "    tq = tqdm(data_loader, desc=desc) if progress else data_loader\n",
    "    for batch_idx, batch in enumerate(tq):\n",
    "        batch: dict = cd.to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(scaler is not None):\n",
    "            outputs: dict = model(batch['inputs'], targets=batch)\n",
    "        loss = outputs['loss']\n",
    "        if progress:\n",
    "            info = [desc] if desc is not None else []\n",
    "            info += ['loss %g' % np.round(cd.asnumpy(loss), 3)]\n",
    "            tq.desc = ' - '.join(info)\n",
    "        if scaler is None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, test_loader, device):\n",
    "    model.eval()\n",
    "    batch = cd.to_device(next(iter(test_loader)), device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['inputs'])\n",
    "    o = cd.asnumpy(outputs)\n",
    "    num = len(o['contours'])\n",
    "    s = int(np.ceil(np.sqrt(num)))\n",
    "    plt.figure(None, (s * 24, s * 13.5))\n",
    "    for idx in range(num):\n",
    "        plt.subplot(s, s, idx + 1)\n",
    "        image = cd.asnumpy(batch['inputs'][idx])\n",
    "        cd.vis.show_detection(Data.unmap(image.transpose(1, 2, 0)), contours=o['contours'][idx],\n",
    "                              contour_linestyle='-', scores=o['scores'][idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "This basic training loop runs for a specified number of epochs and plots an example prediction from the test dataset every now and then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, conf.epochs + 1):\n",
    "    train_epoch(model, train_loader, conf.device, optimizer, f'Epoch {epoch}/{conf.epochs}', scaler, scheduler)\n",
    "    if epoch % 10 == 0:\n",
    "        show_results(model, test_loader, conf.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and FPS measurement\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### Detection and segmentation peformance\n",
    "\n",
    "To evaluate the detection performance and the shape quality of the produced contours we use the harmoc mean of precision and reall\n",
    "$$\\text{F1}_\\tau=\\frac{TP_\\tau}{TP_\\tau+\\frac{1}{2}(FP_\\tau+FN_\\tau)}$$\n",
    "for different *Intersection over Union* (IoU) thresholds $\\tau$.\n",
    "The IoU threshold $\\tau \\in[0,1]$ defines the minimal IoU that is required for two shapes to be counted as a match. \n",
    "Each ground truth shape can be a match for at most one predicted shape.\n",
    "A True Positive $(\\text{TP})$ is a predicted shape that matches a ground truth shape, a False Positive ($\\text{FP}$) is a shape that does not match any ground truth shape and a False Negative ($\\text{FN}$) is a ground truth shape that does not match any predicted shape.\n",
    "\n",
    "### Validation\n",
    "Here, we use the validation split of our dataset to determine which settings yield the best results.\n",
    "The settings that yield the highest average F1 score during validation are used to evaluate the model on the dataset's test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_iou_score(results, iou_threshs=(.5, .6, .7, .8, .9), verbose=True):\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print('iou thresh\\t\\t f1')\n",
    "    for results.iou_thresh in iou_threshs:\n",
    "        scores.append(results.avg_f1)\n",
    "        if verbose:\n",
    "            print(results.iou_thresh, '\\t\\t\\t', np.round(scores[-1], 3))\n",
    "    final_f1 = np.mean(scores).round(3)\n",
    "    if verbose:\n",
    "        print('\\nAverage F1 score:', '\\t', final_f1)\n",
    "    return final_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, use_amp, desc='Eval', progress=True, timing=False):\n",
    "    \"\"\"Evaluate model and return results.\"\"\"\n",
    "    model.eval()\n",
    "    tq = tqdm(data_loader, desc=desc) if progress else data_loader\n",
    "    results = cd.data.LabelMatcherList()\n",
    "    times = []\n",
    "    for batch_idx, batch in enumerate(tq):\n",
    "        batch: dict = cd.to_device(batch, device)\n",
    "        with autocast(use_amp):\n",
    "            if timing:\n",
    "                cd.start_timer('bbbc039', cuda=torch.cuda.is_available())\n",
    "            outputs: dict = model(batch['inputs'])\n",
    "            if timing:\n",
    "                times.append(cd.stop_timer('bbbc039', verbose=False, cuda=torch.cuda.is_available()))\n",
    "        out = cd.asnumpy(outputs)\n",
    "        inp = cd.asnumpy(batch)\n",
    "        targets = inp['targets']\n",
    "        for idx in range(len(targets)):\n",
    "            target = cd.data.channels_first2channels_last(targets[idx])\n",
    "            prediction = cd.data.contours2labels(out['contours'][idx], target.shape[:2])\n",
    "            results.append(cd.data.LabelMatcher(prediction, target))\n",
    "    if len(times) > 0:\n",
    "        cd.print_timing('Average prediction delay', np.mean(times))\n",
    "        print(\"FPS:\", ' ' * 66, (1 / np.mean(times)).round(2), 'frames')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_(model, data_loader, device, use_amp):\n",
    "    \"\"\"Validate model with different settings, keep best performing settings.\"\"\"\n",
    "    best_f1 = 0.\n",
    "    best_settings = {}\n",
    "    for model.score_thresh in conf.val_score_threshs:\n",
    "        for model.nms_thresh in conf.val_nms_threshs:\n",
    "            res = evaluate(model, val_loader, conf.device, use_amp,\n",
    "                           desc=f'Validate (score thresh: {model.score_thresh}, nms_thresh: {model.nms_thresh})')\n",
    "            f1 = avg_iou_score(res, verbose=False)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_settings['score_thresh'] = model.score_thresh\n",
    "                best_settings['nms_thresh'] = model.nms_thresh\n",
    "    for k, v in best_settings.items():\n",
    "        print(f'Best {k}: {v}')\n",
    "        setattr(model, k, v)\n",
    "    print(\"Val f1 score:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_(model, val_loader, conf.device, conf.amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and inference speed\n",
    "Finally, we want to evaluate the model's performance on the test dataset.\n",
    "For this, we measure the average F1 score, as well as FPS (frames per second).\n",
    "The latter is especially important, since practical uses cases of cell detection often involve scans or images with tens or even hundreds of thousands of pixels in height and width.\n",
    "\n",
    "Since the test loader uses a batch size of 1, we can also measure the time that the model needs to predict contours for a single image. This per image delay can be averaged and used to get an estimated count for FPS.\n",
    "In this case all images have size **(520, 696)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate(model, test_loader, conf.device, conf.amp, timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1 = avg_iou_score(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "<hr/>\n",
    "\n",
    "You are now able to use [Contour Proposal Networks](https://arxiv.org/abs/2104.03393) for Cell Detection.\n",
    "Feel free to try different settings, CPN architectures and also datasets.\n",
    "\n",
    "Since the only assumption of the CPN's contour approach are closed object contours, it is applicable to a wide range of detection problems, also outside the biomedical domain that have not been investigated yet.\n",
    "\n",
    "On [celldetection.org](https://celldetection.org) or [https://git.io/JOnWX](https://git.io/JOnWX) you will find additional resources and examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
